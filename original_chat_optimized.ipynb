{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9797d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 PDFs\n",
      "Example file: attention is all you need.pdf\n",
      "First 500 characters:\n",
      " Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to your data folder\n",
    "DATA_PATH = \"Data/\"\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def load_pdfs(data_path):\n",
    "    texts = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            doc = fitz.open(os.path.join(data_path, file))\n",
    "            pdf_text = \"\"\n",
    "            for page in doc:\n",
    "                pdf_text += page.get_text()\n",
    "            texts.append({\"file\": file, \"text\": pdf_text})\n",
    "    return texts\n",
    "\n",
    "# Load PDFs\n",
    "documents = load_pdfs(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {len(documents)} PDFs\")\n",
    "print(\"Example file:\", documents[0][\"file\"])\n",
    "print(\"First 500 characters:\\n\", documents[0][\"text\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecdab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 41\n",
      "Example chunk:\n",
      " Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aid\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = splitter.split_text(doc[\"text\"])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"file\": doc[\"file\"],\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"Example chunk:\\n\", all_chunks[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "987204bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Mangment\\latest\\Mangment\\LLM\\chatwitharchive\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All chunks stored in ChromaDB!\n",
      "Collection size: 41\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"  \n",
    ")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"arxiv_papers\",\n",
    "    embedding_function=embedding_func\n",
    ")\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    collection.add(\n",
    "        documents=[chunk[\"text\"]],\n",
    "        metadatas=[{\"file\": chunk[\"file\"], \"chunk_id\": chunk[\"chunk_id\"]}],\n",
    "        ids=[f'{chunk[\"file\"]}_{chunk[\"chunk_id\"]}']\n",
    "    )\n",
    "\n",
    "print(\"✅ All chunks stored in ChromaDB!\")\n",
    "print(\"Collection size:\", collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21bff2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[9], consum ...\n",
      "From: attention is all you need.pdf\n",
      "\n",
      "Result 2:\n",
      "2\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the func ...\n",
      "From: attention is all you need.pdf\n",
      "\n",
      "Result 3:\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "transla ...\n",
      "From: attention is all you need.pdf\n",
      "\n",
      "Result 4:\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint av ...\n",
      "From: attention is all you need.pdf\n",
      "\n",
      "Result 5:\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperfor ...\n",
      "From: attention is all you need.pdf\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"Explain Transformers in simple terms\"\n",
    "#query = \"Why do we need positional encoding in Transformers?\"\n",
    "\n",
    "# Search in ChromaDB\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(results[\"documents\"][0]):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(doc[:300], \"...\")\n",
    "    print(\"From:\", results[\"metadatas\"][0][i][\"file\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a027b8a",
   "metadata": {},
   "source": [
    "# test with llama instead of mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f9ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers! They're a type of machine learning model that's revolutionized the way we process and understand natural language.\n",
      "\n",
      "**What does \"Transformer\" mean?**\n",
      "\n",
      "The term \"Transformer\" comes from the idea that these models \"transform\" or change the way we think about processing sequences (like words in a sentence) into a more efficient and powerful approach.\n",
      "\n",
      "**How do Transformers work?**\n",
      "\n",
      "Traditional machine learning models, like Recurrent Neural Networks (RNNs), process sequences one step at a time. They look at each word in the sequence, one by one, and use that information to make predictions about the next word.\n",
      "\n",
      "Transformers take a different approach. Instead of looking at individual words, they break down the entire sequence into smaller groups called \"attention heads.\" Each attention head looks at all the other words in the sequence simultaneously, but only considers those that are relevant to it (based on their similarity).\n",
      "\n",
      "This allows the model to capture long-range relationships between words and make predictions about entire sequences, rather than just individual words.\n",
      "\n",
      "**Key Components of Transformers:**\n",
      "\n",
      "1. **Encoder**: This part of the model looks at each word in the sequence and generates a \"context vector\" that represents its importance.\n",
      "2. **Decoder**: This part of the model uses the context vectors to generate the next word in the sequence, based on what it's learned from the previous words.\n",
      "\n",
      "**What are the benefits of Transformers?**\n",
      "\n",
      "Transformers have several advantages over traditional models:\n",
      "\n",
      "1. **Long-range dependencies**: They can capture relationships between distant parts of a sequence.\n",
      "2. **Parallelization**: They can be parallelized more easily, making them faster and more efficient.\n",
      "3. **Flexibility**: They can handle sequences of any length and type (e.g., text, images).\n",
      "\n",
      "**In simple terms...**\n",
      "\n",
      "Imagine you're trying to translate a sentence from English to Spanish. A traditional model would look at each word in the English sentence one by one and try to guess the corresponding Spanish word.\n",
      "\n",
      "A Transformer model, on the other hand, looks at all the words in the entire English sentence at once, but only considers those that are relevant to it. This allows it to capture long-range relationships between words and make more accurate predictions about the Spanish translation.\n",
      "\n",
      "That's Transformers in a nutshell!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert AI/ML assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain Transformers in simple terms.\"},\n",
    "    ]\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f20c66",
   "metadata": {},
   "source": [
    "# test with llama and option parameters ( max token, and temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799ee32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: Neural network architecture that uses attention mechanisms to process sequential data (e.g., text, speech) by transforming input into a fixed-length vector representation.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert AI/ML assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain Transformers in simple terms and do not exceed 150 characters.\"},\n",
    "    ],\n",
    "\n",
    "\n",
    "    options={\n",
    "    \"num_predict\": 150,#,        # Longer technical answers\n",
    "     \"temperature\": 0.1       # More focused, less creative\n",
    "    # \"top_p\": 0.8,             # More precise token selection\n",
    "    # \"repeat_penalty\": 1.05,    # Reduce repetition\n",
    "    # \"num_ctx\": 4096           # Larger context for papers\n",
    "}\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b5f3e",
   "metadata": {},
   "source": [
    "# test withh llama and options and with prompt based on my context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6cd9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Transformer model, the Encoder consists of a stack of identical layers\n",
      "with two sub-layers: 1. Multi-head self-attention mechanism 2. Simple fully\n",
      "connected feed-forward network with residual connections and layer\n",
      "normalization.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "query=\"what is encoder in transformers\"\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an expert AI/ML assistant should reply based on context else reply your inquiry is not available, Keep responses clear, concise and under 100 characters maximum. Be direct and to the point.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Based on this context: {context}\\n\\nQuestion: {query}\\n\\nKeep your answer under 150 characters.\"\n",
    "        }\n",
    "    ],\n",
    "    options={\n",
    "        \"num_predict\": 150,      # Longer technical answers\n",
    "        \"temperature\": 0.1       # More focused, less creative\n",
    "        # \"top_p\": 0.8,          # More precise token selection\n",
    "        # \"repeat_penalty\": 1.05, # Reduce repetition\n",
    "        # \"num_ctx\": 4096        # Larger context for papers\n",
    "    }\n",
    ")\n",
    "\n",
    "#print(response[\"message\"][\"content\"])\n",
    "# Using textwrap (recommended)\n",
    "import textwrap\n",
    "\n",
    "output = response[\"message\"][\"content\"]\n",
    "wrapped_output = textwrap.fill(output, width=80)\n",
    "print(wrapped_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0879e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[9], consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1\n",
      "Encoder and Decoder Stacks\n",
      "Encoder:\n",
      "The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "\n",
      "2\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder:\n",
      "The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2\n",
      "Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\n",
      "\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements\n",
      "We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "References\n"
     ]
    }
   ],
   "source": [
    "#chekc context\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b9647",
   "metadata": {},
   "source": [
    "# adding smart context to simplify the long context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fb8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimized_context(context, question, max_length=300):\n",
    "    \"\"\"More efficient context trimming\"\"\"\n",
    "    # Split into sentences and keep most relevant ones\n",
    "    sentences = context.split('. ')\n",
    "    # Simple keyword matching for relevance\n",
    "    question_words = set(question.lower().split())\n",
    "    scored_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        score = sum(1 for word in sentence.lower().split() if word in question_words)\n",
    "        scored_sentences.append((score, sentence))\n",
    "    \n",
    "    # Sort by relevance and take top sentences\n",
    "    scored_sentences.sort(reverse=True)\n",
    "    return '. '.join([s[1] for s in scored_sentences[:5]])[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bffdbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. This\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GP\n"
     ]
    }
   ],
   "source": [
    "#check smar conext\n",
    "smart_context = get_optimized_context(context, query, 600)\n",
    "print(smart_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11beafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer: Model that uses self-attention mechanism to draw global\n",
      "dependencies between input & output sequences.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "query=\"what transformers in simple terms \"\n",
    "context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "smart_context = get_optimized_context(context, query, 400)\n",
    "smart_context=context\n",
    "max_tokens=200\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an expert AI/ML assistant should reply based on context else reply your inquiry is not available, Keep responses clear, concise and under 100 characters maximum. Be direct and to the point.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Based on this context: {smart_context}\\n\\nQuestion: {query}\\n\\nKeep your answer under {max_tokens} characters.\"\n",
    "        }\n",
    "    ],\n",
    "    options={\n",
    "        \"num_predict\": max_tokens,      # Longer technical answers\n",
    "        \"temperature\": 0.1       # More focused, less creative\n",
    "        # \"top_p\": 0.8,          # More precise token selection\n",
    "        # \"repeat_penalty\": 1.05, # Reduce repetition\n",
    "        # \"num_ctx\": 4096        # Larger context for papers\n",
    "    }\n",
    ")\n",
    "\n",
    "# print(response[\"message\"][\"content\"])\n",
    "import textwrap\n",
    "\n",
    "# Using textwrap (recommended)\n",
    "output = response[\"message\"][\"content\"]\n",
    "wrapped_output = textwrap.fill(output, width=80)\n",
    "print(wrapped_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e2699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging once in your app entrypoint\n",
    "logging.basicConfig(\n",
    "    filename=\"arxiv_bot_optimized.log\",  # log file\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def ask_arxiv_bot_with_sources(question, top_k=10):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    model_start = time.time()  # Start timing\n",
    "    \n",
    "    try:\n",
    "        # Retrieve top-k chunks\n",
    "        results = collection.query(\n",
    "            query_texts=[question],\n",
    "            n_results=top_k\n",
    "        )\n",
    "       \n",
    "        if not results[\"documents\"][0]:\n",
    "            return \"No relevant content found in the PDFs.\"\n",
    "        \n",
    "        context = \"\\n\\n\".join(results[\"documents\"][0])\n",
    "        max_tokens = 400\n",
    "        smart_context = get_optimized_context(context, question, max_tokens)\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.2:3b\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert AI/ML assistant should reply based on context else reply your inquiry is not available, Keep responses clear, concise and under 100 characters maximum. Be direct and to the point.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Based on this context: {smart_context}\\n\\nQuestion: {question}\\n\\nKeep your answer under {max_tokens} characters.\"\n",
    "                }\n",
    "            ],\n",
    "            options={\n",
    "                \"num_predict\": max_tokens,      # Longer technical answers\n",
    "                \"temperature\": 0.1       # More focused, less creative\n",
    "                # \"top_p\": 0.8,          # More precise token selection\n",
    "                # \"repeat_penalty\": 1.05, # Reduce repetition\n",
    "                # \"num_ctx\": 4096        # Larger context for papers\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        generated_text = response[\"message\"][\"content\"]  # Extract text using dict access\n",
    "        logging.info(\"end time %s\", time.time() - model_start)\n",
    "        \n",
    "        # Collect sources\n",
    "        sources = [md[\"file\"] for md in results[\"metadatas\"][0]]\n",
    "        \n",
    "        return f\"{generated_text}\\n\\nSources: {', '.join(set(sources))}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c1859fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pdfs_to_collection(file_paths):\n",
    "    for path in file_paths:\n",
    "        # path is now a string path\n",
    "        doc = fitz.open(path)  \n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        chunks = splitter.split_text(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            collection.add(\n",
    "                documents=[chunk],\n",
    "                metadatas=[{\"file\": os.path.basename(path), \"chunk_id\": i}],\n",
    "                ids=[f'{os.path.basename(path)}_{i}']\n",
    "            )\n",
    "    return \" PDFs added to collection!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fde79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Arxiv RAG AI optimized Assistant\")\n",
    "    \n",
    "    # Upload PDFs (fixed)\n",
    "    pdf_input = gr.File(file_types=[\".pdf\"], type=\"filepath\", label=\"Upload new PDFs\", file_count=\"multiple\")\n",
    "    \n",
    "    # Question input\n",
    "    question_input = gr.Textbox(lines=2, placeholder=\"Type your question here...\", label=\"Ask a question\")\n",
    "    \n",
    "    # Answer output\n",
    "      # Answer output\n",
    "    answer_output = gr.Textbox(\n",
    "    label=\"Answer\", \n",
    "    lines=10,  # Increase from default ~3 to 10 lines\n",
    "    max_lines=20,  # Allow expansion up to 20 lines\n",
    "    show_copy_button=True  # Add copy button for long answers\n",
    ")\n",
    "    # Buttons\n",
    "    upload_btn = gr.Button(\"Add PDFs\")\n",
    "    ask_btn = gr.Button(\"Ask Question\")\n",
    "\n",
    "    upload_btn.click(lambda files: add_pdfs_to_collection(files), inputs=[pdf_input], outputs=[answer_output])\n",
    "    ask_btn.click(\n",
    "    fn=ask_arxiv_bot_with_sources,  # must return a string\n",
    "    inputs=[question_input],\n",
    "    outputs=[answer_output]\n",
    ")\n",
    "\n",
    "    \n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more enahcememnt \n",
    "# chunking by page or by paragpraph based on my data\n",
    "# example AI engineer guide by page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890d5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
